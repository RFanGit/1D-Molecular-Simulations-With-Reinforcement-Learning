{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2b132d-144b-4eb3-a584-fbdd8e69bbd5",
   "metadata": {},
   "source": [
    "In this notebook, we run and compare results from the analytical solution and our reinforcement learning solution. Here are the functions we use in our programs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "251b5fc4-acfc-45c8-b77e-8ea2186a4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "\n",
    "from gymnasium import spaces\n",
    "\n",
    "class ReplayMemory: ##Replay memory to store past experiences to learn from\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module): ##Our controller is a neural network that takes in input dimension (the history length) and outputs a single number\n",
    "    ##Because output has numerical meaning (5 is in fact close to 4 and 6 as outputs) we'll output a single integer instead of a number of categories\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential( ##2 layers of 64 neurons. Maybe we need more complexity later.\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def step_function(model, optimizer, loss_fn, states, outs):\n",
    "    model.train()\n",
    "    \n",
    "    predict_output = model(states)\n",
    "    loss = loss_fn(predict_output, outs)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "##Our memory contains states and the optimal output value\n",
    "def train_from_memory(model, optimizer, loss_fn, replay_memory, batch_size):\n",
    "    if len(replay_memory) < batch_size:\n",
    "        return 0\n",
    "\n",
    "    batch = replay_memory.sample(batch_size)\n",
    "    state_list, output_list = zip(*batch)\n",
    "\n",
    "    #print (state_list)\n",
    "    #print (\"The state list is \", state_list)\n",
    "    #print (\"The output list is \", output_list)\n",
    "\n",
    "    states = torch.tensor(np.array(state_list), dtype=torch.float32)\n",
    "    #print (\"The state tensor is \", states)\n",
    "    outputs = torch.tensor(np.array(output_list), dtype=torch.float32).unsqueeze(1)\n",
    "    #print (\"The output tensor is \", outputs)\n",
    "\n",
    "    loss = step_function(model, optimizer, loss_fn, states, outputs)\n",
    "    return loss\n",
    "\n",
    "class OneMoleculeEnv(gym.Env):\n",
    "    def __init__(self, initial_value=10, molecule_lifetime = 1, dt = 0.1, max_steps=100, history_length=5, target_value=10, obs_cap=100, render_mode=None):\n",
    "        super(OneMoleculeEnv, self).__init__()\n",
    "\n",
    "        self.initial_value = initial_value\n",
    "        self.prob_death = np.exp(-dt/molecule_lifetime)\n",
    "        self.dt = dt\n",
    "        self.max_steps = max_steps\n",
    "        self.history_length = history_length\n",
    "        self.target_value = target_value\n",
    "        self.obs_cap = obs_cap  # Cap for the observation space\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.current_value = initial_value\n",
    "        self.current_step = 0\n",
    "        self.decays = 0\n",
    "        self.ideal_action = 0\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(obs_cap)  # the number of molecules to send in ranges from 0 to the cap\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=obs_cap, shape=(history_length,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Initialize the history of values\n",
    "        self.history = np.full(history_length, initial_value, dtype=np.float32)\n",
    "\n",
    "        # Pre-generate random numbers to avoid generating them at each step\n",
    "        self.random_numbers = np.random.rand(max_steps)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            self.random_numbers = np.random.rand(self.max_steps)  # Re-generate random numbers if seeded\n",
    "        self.current_value = self.initial_value\n",
    "        self.current_step = 0\n",
    "        self.history = np.full(self.history_length, self.initial_value, dtype=np.float32)\n",
    "        return self.history, {}\n",
    "\n",
    "    def _ensure_random_numbers(self):\n",
    "        # Reset the random numbers if current_step exceeds max_steps\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.random_numbers = np.random.rand(self.max_steps)\n",
    "            self.current_step = 0  # Reset current step to start fresh\n",
    "\n",
    "    # First, ensure enough random numbers are available\n",
    "    # The probability a molecule has decayed in the time interval given is dependent on dt and the lifetime.\n",
    "    # Decrease the value with a number drawn from a binomial distribution\n",
    "    # This reflects each molecule having a finite probability of decaying in the timestep\n",
    "    # The probability a molecule has decayed in the time interval given is dependent on dt and the lifetime.\n",
    "    # Then add molecules.\n",
    "    # We then update the history, increase the steps, and calculate the error (reward)\n",
    "    # Then return the standard gymnasium properties.\n",
    "    def step(self, action):\n",
    "        # Ensure enough random numbers are available\n",
    "        self._ensure_random_numbers()\n",
    "\n",
    "        self.decays = -np.random.binomial((int)(self.current_value), self.prob_death, 1).item()\n",
    "        self.current_value += self.decays\n",
    "        self.ideal_action = self.target_value - self.current_value\n",
    "        self.current_value += action\n",
    "        #print (\"The options for self current value are \")\n",
    "        #print (self.current_value, ' ', self.obs_cap)\n",
    "        #print (self.obs_cap, ' vs ', self.current_value)\n",
    "\n",
    "        self.current_value = min(self.current_value, self.obs_cap)\n",
    "\n",
    "        self.history[:-1] = self.history[1:]\n",
    "        self.history[-1] = np.float32(self.current_value)  # Ensure it's a scalar\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        reward = -float((self.current_value - self.target_value) ** 2)  # Ensure the reward is a float\n",
    "\n",
    "        return self.history, reward, done, done, self.ideal_action\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            print(f\"Step: {self.current_step}, Value: {self.current_value}, History: {self.history}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "def train_molecule_controller(model, optimizer, loss_fn, replay_memory, steps, target, molecule_lifetime, dt, history_length, observable_indices, RUN_SEED, batch_size):\n",
    "    ##Here we choose our environment\n",
    "    env = OneMoleculeEnv(\n",
    "        initial_value=target,\n",
    "        molecule_lifetime=molecule_lifetime,\n",
    "        dt=dt,\n",
    "        max_steps=steps,\n",
    "        history_length=history_length,\n",
    "        target_value=target,\n",
    "        obs_cap=target*3,\n",
    "        render_mode=None\n",
    "    )\n",
    "    \n",
    "    RUN_SEED = 0\n",
    "    \n",
    "    observation, info = env.reset(seed=RUN_SEED)\n",
    "    observation_tensor = torch.tensor(observation[observable_indices], dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    for step in range(steps):\n",
    "\n",
    "        ##we compute our optimal action using the most \n",
    "        #print (observation[observable_indices])\n",
    "        with torch.no_grad():\n",
    "            action = model(observation_tensor)\n",
    "        action[action <0] = 0 ##controller cannot remove molecules\n",
    "        rounded_action = torch.round(action).int().item() ##round the action to the nearest integer\n",
    "        observation, reward, done, truncated, info = env.step(rounded_action)\n",
    "        observation_tensor = torch.tensor(observation[observable_indices], dtype=torch.float32).unsqueeze(0)\n",
    "        #print (\"Action taken \", rounded_action, ' ideal action ', info, ' current observation ', observation_tensor)\n",
    "        replay_memory.add((observation_tensor, info))\n",
    "\n",
    "        q_error = train_from_memory(model, optimizer, loss_fn, replay_memory, batch_size)\n",
    "    env.close()\n",
    "    return 0\n",
    "\n",
    "def test_molecule_controller(model, optimizer, loss_fn, replay_memory, steps, target, molecule_lifetime, dt, history_length, observable_indices, RUN_SEED, batch_size):\n",
    "    ##Here we choose our environment\n",
    "    env = OneMoleculeEnv(\n",
    "        initial_value=target,\n",
    "        molecule_lifetime=molecule_lifetime,\n",
    "        dt=dt,\n",
    "        max_steps=steps,\n",
    "        history_length=history_length,\n",
    "        target_value=target,\n",
    "        obs_cap=target*3,\n",
    "        render_mode=None\n",
    "    )\n",
    "    \n",
    "    rewards = []\n",
    "    training_error = []\n",
    "    RUN_SEED = 0\n",
    "    \n",
    "    observation, info = env.reset(seed=RUN_SEED)\n",
    "    observation_tensor = torch.tensor(observation[observable_indices], dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    for step in range(steps):\n",
    "\n",
    "        ##we compute our optimal action using the most \n",
    "        #print (observation[observable_indices])\n",
    "        with torch.no_grad():\n",
    "            action = model(observation_tensor)\n",
    "        action[action <0] = 0 ##controller cannot remove molecules\n",
    "        rounded_action = torch.round(action).int().item() ##round the action to the nearest integer\n",
    "        observation, reward, done, truncated, info = env.step(rounded_action)\n",
    "        observation_tensor = torch.tensor(observation[observable_indices], dtype=torch.float32).unsqueeze(0)\n",
    "        #print (\"Action taken \", rounded_action, ' ideal action ', info, ' current observation ', observation_tensor)\n",
    "        replay_memory.add((observation_tensor, info))\n",
    "\n",
    "        q_error = train_from_memory(model, optimizer, loss_fn, replay_memory, batch_size)\n",
    "        if q_error > 0:\n",
    "            training_error.append(q_error)\n",
    "        rewards.append(reward)\n",
    "    env.close()\n",
    "    return np.array(rewards), np.array(training_error)\n",
    "\n",
    "def plot_with_smoothing(data, xlabel, ylabel, title, smoothing_window=1):\n",
    "    #- smoothing_window: int, the number of observations to average out for smoothing. If set to 1, no smoothing is applied.\n",
    "    # Apply smoothing if smoothing_window is greater than 1\n",
    "    if smoothing_window > 1:\n",
    "        smoothed_data = np.convolve(data, np.ones(smoothing_window)/smoothing_window, mode='valid')\n",
    "        # Plot the smoothed data with original indices adjusted for the window\n",
    "        plt.plot(range(smoothing_window-1, len(data)), smoothed_data, label=f'Smoothed (window={smoothing_window})')\n",
    "    else:\n",
    "        # If no smoothing is required, just plot the original data\n",
    "        plt.plot(data, label='Original Data')\n",
    "\n",
    "    # Plot the original data for comparison\n",
    "    plt.plot(data, alpha=0.3, label='Original Data (Transparent)')\n",
    "    \n",
    "    # Add labels and a legend\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def optimal_action_th(molecules, target, molecule_lifetime, dt):\n",
    "    prob_death = np.exp(-dt/molecule_lifetime)\n",
    "    optimal_action = target - molecules*(1 - prob_death)\n",
    "    return optimal_action\n",
    "\n",
    "def optimal_solution(steps, target, molecule_lifetime, dt = 0.5, RUN_SEED = 0):\n",
    "    ##Here we choose our environment\n",
    "    env = OneMoleculeEnv(\n",
    "        initial_value=target,\n",
    "        molecule_lifetime=molecule_lifetime,\n",
    "        dt=dt,\n",
    "        max_steps=steps,\n",
    "        history_length=1,\n",
    "        target_value=target,\n",
    "        obs_cap=target*3,\n",
    "        render_mode=None\n",
    "    )\n",
    "        \n",
    "    rewards = []\n",
    "    observation, info = env.reset(seed=RUN_SEED)\n",
    "    \n",
    "    for step in range(steps):\n",
    "\n",
    "        ##we compute our optimal action using the most \n",
    "        action = optimal_action_th(observation[-1], target, molecule_lifetime, dt)\n",
    "        rounded_action = (int)(np.round(action))\n",
    "        observation, reward, done, truncated, info = env.step(rounded_action)\n",
    "        #print (\"Reward \", reward, \" Observation \", observation, \" Action \", action)\n",
    "\n",
    "        ##The error is the negative of the reward\n",
    "        rewards.append(reward)\n",
    "    env.close()\n",
    "    return np.array(rewards)\n",
    "\n",
    "def end_average(arr, number):\n",
    "    return np.mean(arr[-number:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ba5ab-b972-469d-a4d4-ebb1f4c65292",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10000\n",
    "target = 20\n",
    "molecule_lifetime = 1.0\n",
    "dt = 0.5\n",
    "history_length = 1\n",
    "observable_indices = 0\n",
    "\n",
    "RUN_SEED = 0\n",
    "batch_size = 200\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "train_steps = 100000\n",
    "model = NeuralNetwork(input_dim = 1)\n",
    "learning_rate = 1e-3\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "replay_memory = ReplayMemory(capacity=2000)\n",
    "\n",
    "train_molecule_controller(model, optimizer, loss_fn, replay_memory, train_steps, target, molecule_lifetime, dt, history_length, observable_indices, RUN_SEED, batch_size)\n",
    "rewards_rl, training_error = test_molecule_controller(model, optimizer, loss_fn, replay_memory, steps, target, molecule_lifetime, dt, history_length, observable_indices, RUN_SEED, batch_size)\n",
    "end_time = time.time()\n",
    "rl_time = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "rewards_th = optimal_solution(steps, target, molecule_lifetime, dt, RUN_SEED = 0)\n",
    "end_time = time.time()\n",
    "th_time = end_time - start_time\n",
    "\n",
    "print (\"The reinforcement learning algorithm took \", rl_time, \" while the theoretical solution took \", th_time)\n",
    "\n",
    "plot_with_smoothing(-rewards_rl[-1000:], 'Steps', 'Variability (RL)', 'Last 1000 Steps (RL)', smoothing_window=20)\n",
    "plot_with_smoothing(-rewards_th[-1000:], 'Steps', 'Variability (TH)', 'Last 1000 Steps (TH)', smoothing_window=20)\n",
    "\n",
    "print (\"The average variability over the last 10000 observations was (rl) \", end_average(-rewards_rl, 10000))\n",
    "print (\"The average variability over the last 5000 observations was (rl) \", end_average(-rewards_rl, 5000))\n",
    "\n",
    "print (\"The average variability over the last 10000 observations was (th) \", end_average(-rewards_th, 10000))\n",
    "print (\"The average variability over the last 5000 observations was (th) \", end_average(-rewards_th, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f866dc7-a4b4-45d9-862a-d00e56e7fd66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
