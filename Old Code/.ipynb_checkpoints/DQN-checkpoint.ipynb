{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2f5910-b07b-448d-8bc9-aa8dd10d514c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "File \u001b[1;32mD:\\Users\\raymo\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:132\u001b[0m\n\u001b[0;32m    130\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 132\u001b[0m     res \u001b[38;5;241m=\u001b[39m kernel32\u001b[38;5;241m.\u001b[39mLoadLibraryExW(dll, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0x00001100\u001b[39m)\n\u001b[0;32m    133\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "\n",
    "from gymnasium import spaces\n",
    "\n",
    "class OneMoleculeEnv(gym.Env):\n",
    "    def __init__(self, initial_value=10, molecule_lifetime = 1, dt = 0.1, max_steps=100, history_length=5, target_value=10, obs_cap=40, render_mode=None):\n",
    "        super(OneMoleculeEnv, self).__init__()\n",
    "\n",
    "        self.initial_value = initial_value\n",
    "        self.molecule_lifetime = p_death\n",
    "        self.dt = dt\n",
    "        self.max_steps = max_steps\n",
    "        self.history_length = history_length\n",
    "        self.target_value = target_value\n",
    "        self.obs_cap = obs_cap  # Cap for the observation space\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.current_value = initial_value\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(2)  # Two actions: 0 (do nothing), 1 (increase by 1)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=obs_cap, shape=(history_length,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Initialize the history of values\n",
    "        self.history = np.full(history_length, initial_value, dtype=np.float32)\n",
    "\n",
    "        # Pre-generate random numbers to avoid generating them at each step\n",
    "        self.random_numbers = np.random.rand(max_steps)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            self.random_numbers = np.random.rand(self.max_steps)  # Re-generate random numbers if seeded\n",
    "        self.current_value = self.initial_value\n",
    "        self.current_step = 0\n",
    "        self.history = np.full(self.history_length, self.initial_value, dtype=np.float32)\n",
    "        return self.history, {}\n",
    "\n",
    "    def _ensure_random_numbers(self):\n",
    "        # Reset the random numbers if current_step exceeds max_steps\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.random_numbers = np.random.rand(self.max_steps)\n",
    "            self.current_step = 0  # Reset current step to start fresh\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        if action == 1:\n",
    "            self.current_value += 1\n",
    "        # Ensure enough random numbers are available\n",
    "        self._ensure_random_numbers()\n",
    "\n",
    "        # Decrease the value with a number drawn from a binomial distribution\n",
    "        # This reflects each molecule having a finite probability of decaying in the timestep\n",
    "        # The probability a molecule has decayed in the time interval given is\n",
    "        self.prob_death = np.exp(-dt/molecule_lifetime)\n",
    "        \n",
    "        self.current_value += -np.random.binomial(self.current_value, self.p_death, 1)\n",
    "\n",
    "        # Cap the current value to obs_cap\n",
    "        self.current_value = min(self.current_value, self.obs_cap)\n",
    "\n",
    "        # Update the history using a simple list rotation\n",
    "        self.history[:-1] = self.history[1:]\n",
    "        self.history[-1] = np.float32(self.current_value)  # Ensure it's a scalar\n",
    "\n",
    "        # Increment step count\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Check if done\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = -float((self.current_value - self.target_value) ** 2)  # Ensure the reward is a float\n",
    "\n",
    "        return self.history, reward, done, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            print(f\"Step: {self.current_step}, Value: {self.current_value}, History: {self.history}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# Register the environment\n",
    "gym.envs.registration.register(\n",
    "    id='OneMoleculeEnv-v0',\n",
    "    entry_point=OneMoleculeEnv,\n",
    "    max_episode_steps=1000000,\n",
    ")\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "total_time_train = 0\n",
    "total_time_predict = 0\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def select_action_with_epsilon_greedy(Q_value, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(2)\n",
    "    else:\n",
    "        return np.argmax(Q_value)\n",
    "\n",
    "def step_function(model, target_model, optimizer, loss_fn, s, a, r, d, ns, discount):\n",
    "    model.train()\n",
    "    \n",
    "    qs = model(s)\n",
    "    qa = qs.gather(1, a)\n",
    "\n",
    "    done_mask = (d == 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_qs = target_model(ns)\n",
    "        max_next_qs = torch.max(next_qs, dim=1, keepdim=True)[0]\n",
    "        target = r + discount * torch.where(done_mask, max_next_qs, torch.tensor(0))\n",
    "    \n",
    "    loss = loss_fn(qa, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def train_from_memory(model, target_model, optimizer, loss_fn, replay_memory, batch_size, discount):\n",
    "    if len(replay_memory) < batch_size:\n",
    "        return 0\n",
    "\n",
    "    batch = replay_memory.sample(batch_size)\n",
    "    state_list, action_list, reward_list, next_state_list, done_list = zip(*batch)\n",
    "\n",
    "    states = torch.tensor(np.array(state_list), dtype=torch.float32)\n",
    "    actions = torch.tensor(np.array(action_list), dtype=torch.int64).unsqueeze(1)\n",
    "    rewards = torch.tensor(np.array(reward_list), dtype=torch.float32).unsqueeze(1)\n",
    "    next_states = torch.tensor(np.array(next_state_list), dtype=torch.float32)\n",
    "    dones = torch.tensor(np.array(done_list), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    loss = step_function(model, target_model, optimizer, loss_fn, states, actions, rewards, dones, next_states, discount)\n",
    "    return loss\n",
    "    \n",
    "#When to stop training\n",
    "def terminate_performance(rewards_overtime, max_reward, consistent_perf):\n",
    "    # Get the last 3 elements\n",
    "    last_three = rewards_overtime[-consistent_perf:]\n",
    "\n",
    "    # Calculate the average\n",
    "    average = np.mean(last_three)\n",
    "    if average > max_reward -1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "##Saving and loading our results\n",
    "def save_model(model, optimizer, filename):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, filename)\n",
    "    print(f\"Model and optimizer state saved to {filename}\")\n",
    "\n",
    "def save_arrays(rewards_overtime, error_over_episodes, rewards_filename, errors_filename):\n",
    "    np.savetxt(rewards_filename, rewards_overtime, delimiter=',')\n",
    "    np.savetxt(errors_filename, error_over_episodes, delimiter=',')\n",
    "    print(f\"Rewards saved to {rewards_filename}\")\n",
    "    print(f\"Q errors saved to {errors_filename}\")\n",
    "\n",
    "##This is dependent on the gymnasium environment\n",
    "def environment_train(env, model, target_model, optimizer, loss_fn, replay_memory, RUN_SEED, MAX_STEPS, VISIBLE, batch_size, discount, epsilon_start, epsilon_end, epsilon_decay, total_time_train, total_time_predict):\n",
    "    observation, info = env.reset(seed=RUN_SEED)\n",
    "    total_reward = 0\n",
    "\n",
    "    rewards_overtime = []\n",
    "    error_over_episodes = []\n",
    "    error_in_episode = []\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    steps = 0\n",
    "    play = True\n",
    "\n",
    "    ##Implementing a target network to make training more stable with DQN\n",
    "    target_update_freq = 100\n",
    "\n",
    "    while play:\n",
    "        start_time = time.time()\n",
    "        observation_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            Q_values = model(observation_tensor)\n",
    "        action = select_action_with_epsilon_greedy(Q_values.numpy(), epsilon)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        replay_memory.add((observation, action, reward, next_observation, done))\n",
    "        observation = next_observation\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time_predict += (end_time - start_time)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        q_error = train_from_memory(model, target_model, optimizer, loss_fn, replay_memory, batch_size, discount)\n",
    "        if q_error > 0:\n",
    "            error_in_episode.append(q_error)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        total_time_train += (end_time - start_time)\n",
    "        \n",
    "        total_reward += reward\n",
    "        if done or truncated:\n",
    "            average_error = np.mean(error_in_episode) if error_in_episode else 0\n",
    "            print(\"The reward for this game was\", total_reward)\n",
    "            print(\"The average Q error for this game was\", average_error)\n",
    "            error_over_episodes.append(average_error)\n",
    "            rewards_overtime.append(total_reward)\n",
    "            total_reward = 0\n",
    "            observation, info = env.reset()\n",
    "\n",
    "        if steps % target_update_freq == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_end)\n",
    "        steps += 1\n",
    "        if steps > MAX_STEPS:\n",
    "            play = False\n",
    "        if terminate_performance(rewards_overtime, 500, 5):\n",
    "            play = False\n",
    "    env.close()\n",
    "    return rewards_overtime, error_over_episodes, total_time_train, total_time_predict, steps\n",
    "\n",
    "##Here we choose our environment\n",
    "env = gym.make('OneMoleculeEnv-v0', initial_value=10, max_steps=100000, history_length=5, target_value=15, obs_cap=100)\n",
    "print('Environment created!')\n",
    "    \n",
    "model = NeuralNetwork(input_dim = 6, output_dim = 3)\n",
    "learning_rate = 1e-3\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "target_model = NeuralNetwork(input_dim = 6, output_dim = 3)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "print('model created!')\n",
    "\n",
    "replay_memory = ReplayMemory(capacity=2000)\n",
    "\n",
    "RUN_SEED = 0\n",
    "MAX_STEPS = 100000\n",
    "VISIBLE = False\n",
    "batch_size = 64\n",
    "discount = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "rewards_overtime, error_overtime, total_time_train, total_time_predict, steps = environment_train(env, model, target_model, optimizer, loss_fn, replay_memory, RUN_SEED, MAX_STEPS, VISIBLE, batch_size, discount, epsilon_start, epsilon_end, epsilon_decay, total_time_train, total_time_predict)\n",
    "print (\"Terminated learning at step \", steps)\n",
    "print(\"The rewards over time are\", rewards_overtime)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "model_filename = 'one_molecule_model.pth'\n",
    "save_model(model, optimizer, model_filename)\n",
    "\n",
    "rewards_filename = 'molecule_rewards.txt'\n",
    "errors_filename = 'molecule_error.txt'\n",
    "save_arrays(np.array(rewards_overtime), np.array(error_overtime), rewards_filename, errors_filename)\n",
    "\n",
    "size = len(rewards_overtime)\n",
    "episode_num = np.arange(size)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episode_num, rewards_overtime, 'r-', lw=2)\n",
    "plt.title('Rewards vs Episode Count', fontsize = 30)\n",
    "plt.xlabel('Episodes', fontsize = 20)\n",
    "plt.ylabel('Rewards', fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episode_num, error_overtime, 'k-', lw=2)\n",
    "plt.title('Q Error Vs Episode Count', fontsize = 30)\n",
    "plt.xlabel('Episodes', fontsize = 20)\n",
    "plt.ylabel('Q Error', fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "print(f\"Total time taken by training: {total_time_train} seconds\")\n",
    "print(f\"Total time taken by predicting: {total_time_predict} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c1f80-84d1-4d55-b5a7-42e71c7d9a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
